{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "0.12.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "223px",
        "width": "193px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Notebook contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "226px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "movie_lens_fair_svae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqrrncx06Ejf",
        "colab_type": "text"
      },
      "source": [
        "# Fairness Aware Sequential Variational Auto Encoder\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-W-jWh6Ejg",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A7dHsN8C6Ejh",
        "colab_type": "code",
        "outputId": "6df4678d-e7f6-430b-b2a5-815b05ddbda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import functools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from random import randint\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbLuo7K_6Ejk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KsmhEHs6Ejm",
        "colab_type": "text"
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knf3mAvX6Ejn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### change `DATA_DIR` to the location where the dataset sits\n",
        "### compatible datasets: ML-1M, Netflix-full\n",
        "\n",
        "hyper_params = {\n",
        "    'data_base': '/content/drive/My Drive/thesis/Sequential_Vae/ml-20m/', # Don't remove the '/' at the end please :)\n",
        "    'project_name': 'svae_ml20m',\n",
        "    # 'data_base': 'saved_data/netflix-full/',\n",
        "    # 'project_name': 'svae_netflix_full',\n",
        "    'model_file_name': '',\n",
        "    'log_file': '',\n",
        "    'history_split_test': [0.8, 0.2], # Part of test history to train on : Part of test history to test\n",
        "\n",
        "    'learning_rate': 0.01, # learning rate is required only if optimizer is adagrad\n",
        "    'optimizer': 'adam',\n",
        "    'weight_decay': float(5e-3),\n",
        "\n",
        "    'epochs': 15,\n",
        "    'batch_size': 1, # Needs to be 1, because we don't pack multiple sequences in the same batch\n",
        "    \n",
        "    'item_embed_size': 256,\n",
        "    'rnn_size': 200,\n",
        "    'hidden_size': 150,\n",
        "    'latent_size': 64,\n",
        "    'loss_type': 'next_k', # [predict_next, same, prefix, postfix, exp_decay, next_k]\n",
        "    'next_k': 4,\n",
        "    'noise': 0,\n",
        "\n",
        "    'number_users_to_keep': 1000000000,\n",
        "    'batch_log_interval': 10000,\n",
        "    'train_cp_users': 200,\n",
        "    'exploding_clip': 0.25,\n",
        "}\n",
        "\n",
        "file_name = '_optimizer_' + str(hyper_params['optimizer'])\n",
        "if hyper_params['optimizer'] == 'adagrad':\n",
        "    file_name += '_lr_' + str(hyper_params['learning_rate'])\n",
        "file_name += '_weight_decay_' + str(hyper_params['weight_decay'])\n",
        "file_name += '_loss_type_' + str(hyper_params['loss_type'])\n",
        "file_name += '_item_embed_size_' + str(hyper_params['item_embed_size'])\n",
        "file_name += '_rnn_size_' + str(hyper_params['rnn_size'])\n",
        "file_name += '_latent_size_' + str(hyper_params['latent_size'])\n",
        "\n",
        "log_file_root = \"/content/drive/My Drive/thesis/Sequential_Vae/ml-20m/saved_logs_fin/\" # Don't remove the '/' at the end please :)\n",
        "model_file_root = \"/content/drive/My Drive/thesis/Sequential_Vae/ml-20m/saved_models_fin/\" # Don't remove the '/' at the end please :)\n",
        "\n",
        "if not os.path.isdir(log_file_root): os.mkdir(log_file_root)\n",
        "if not os.path.isdir(model_file_root): os.mkdir(model_file_root)\n",
        "hyper_params['log_file'] = log_file_root + hyper_params['project_name'] + '_log' + file_name + '.txt'\n",
        "hyper_params['model_file_name'] = model_file_root + hyper_params['project_name'] + '_model' + file_name + '.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mDyPrrx6Ejq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = hyper_params['data_base']\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg') # Path where preprocessed data will be saved\n",
        "hyper_params['data_base'] += 'pro_sg/'\n",
        "\n",
        "if not os.path.isdir(pro_dir): # We don't want to keep preprocessing every time we run the notebook\n",
        "    cols = ['userId', 'movieId', 'rating', 'timestamp']\n",
        "    dtypes = {'userId': 'int', 'movieId': 'int', 'timestamp': 'int', 'rating': 'int'}\n",
        "    #raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), sep='::', names=cols, parse_dates=['timestamp'])\n",
        "    raw_data =pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)\n",
        "    print (raw_data)\n",
        "    max_seq_len = 1000\n",
        "    n_heldout_users = 10000 # If total users = N; train_users = N - 2*heldout; test_users & val_users = heldout\n",
        "\n",
        "    # binarize the data (only keep ratings >= 4)\n",
        "    raw_data = raw_data[raw_data['rating'] > 3.5]\n",
        "\n",
        "    # Remove users with greater than $max_seq_len number of watched movies\n",
        "    raw_data = raw_data.groupby([\"userId\"]).filter(lambda x: len(x) <= max_seq_len)\n",
        "\n",
        "    # Sort data values with the timestamp\n",
        "    raw_data = raw_data.groupby([\"userId\"]).apply(lambda x: x.sort_values([\"timestamp\"], ascending = True)).reset_index(drop=True)\n",
        "\n",
        "    print(raw_data.head(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGgWrPJ7oIdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
        "    count = playcount_groupbyid.size()\n",
        "    print (count)\n",
        "    return count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSs052v_oZpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'movieId')\n",
        "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "        print(itemcount)\n",
        "    \n",
        "    # Only keep the triplets for users who clicked on at least min_uc items\n",
        "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'userId')\n",
        "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
        "        print(usercount)\n",
        "    # Update both usercount and itemcount after filtering\n",
        "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
        "    print (usercount)\n",
        "    return tp, usercount, itemcount\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7HhXyiU6Ejs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('userId')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            # idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "            idx[int((1.0 - test_prop) * n_items_u):] = True\n",
        "            # print(idx)\n",
        "            \n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"%d users sampled\" % i)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "    \n",
        "    return data_tr, data_te\n",
        "\n",
        "def numerize(tp):\n",
        "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
        "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
        "    ra = list(map(lambda x: x, tp['rating']))\n",
        "    ret =  pd.DataFrame(data={'uid': uid, 'sid': sid, 'rating': ra}, columns=['uid', 'sid', 'rating'])\n",
        "    ret['rating'] = ret['rating'].apply(pd.to_numeric)\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl2FyQ176Eju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(pro_dir): # We don't want to keep preprocessing every time we run the notebook\n",
        "\n",
        "    raw_data, user_activity, item_popularity = filter_triplets(raw_data)\n",
        "\n",
        "    sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
        "\n",
        "    print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
        "          (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))\n",
        "\n",
        "    unique_uid = user_activity.index\n",
        "\n",
        "    np.random.seed(98765)\n",
        "    idx_perm = np.random.permutation(unique_uid.size)\n",
        "    unique_uid = unique_uid[idx_perm]\n",
        "\n",
        "    # create train/validation/test users\n",
        "    n_users = unique_uid.size\n",
        "\n",
        "    tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "    vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "    te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "    train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
        "\n",
        "    unique_sid = pd.unique(train_plays['movieId'])\n",
        "\n",
        "    show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "    profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "    if not os.path.exists(pro_dir):\n",
        "        os.makedirs(pro_dir)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "        for sid in unique_sid:\n",
        "            f.write('%s\\n' % sid)\n",
        "\n",
        "    vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "    vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "    vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "    test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "    test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "    test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "    train_data = numerize(train_plays)\n",
        "    train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "    vad_data_tr = numerize(vad_plays_tr)\n",
        "    vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "    vad_data_te = numerize(vad_plays_te)\n",
        "    vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "    test_data_tr = numerize(test_plays_tr)\n",
        "    test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "    test_data_te = numerize(test_plays_te)\n",
        "    test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q63-THuRYgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_data.head(n=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxF_irwhz4H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_histogram(data):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots()\n",
        "    print(max(data))\n",
        "    plt.hist(data,(max(data)))\n",
        "    #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EldnVtz6Ejw",
        "colab_type": "text"
      },
      "source": [
        "# Utlity functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvLKq4Btz6Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot_histogram(raw_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4iURa1I-6Ejw",
        "colab_type": "code",
        "outputId": "b283afc2-387a-47ec-ea91-6e77647095ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "LongTensor = torch.LongTensor\n",
        "FloatTensor = torch.FloatTensor\n",
        "\n",
        "is_cuda_available = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda_available: \n",
        "    print(\"Using CUDA...\\n\")\n",
        "    LongTensor = torch.cuda.LongTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor\n",
        "    \n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def save_obj_json(obj, name):\n",
        "    with open(name + '.json', 'w') as f:\n",
        "        json.dump(obj, f)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_obj_json(name):\n",
        "    with open(name + '.json', 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def file_write(log_file, s):\n",
        "    print(s)\n",
        "    f = open(log_file, 'a')\n",
        "    f.write(s+'\\n')\n",
        "    f.close()\n",
        "\n",
        "def clear_log_file(log_file):\n",
        "    f = open(log_file, 'w')\n",
        "    f.write('')\n",
        "    f.close()\n",
        "\n",
        "def pretty_print(h):\n",
        "    print(\"{\")\n",
        "    for key in h:\n",
        "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
        "    print('}\\n')\n",
        "    \n",
        "def plot_len_vs_ndcg(len_to_ndcg_at_100_map):\n",
        "    \n",
        "    lens = list(len_to_ndcg_at_100_map.keys())\n",
        "    lens.sort()\n",
        "    X, Y = [], []\n",
        "    \n",
        "    for le in lens:\n",
        "        X.append(le)\n",
        "        ans = 0.0\n",
        "        for i in len_to_ndcg_at_100_map[le]: ans += float(i)\n",
        "        ans = ans / float(len(len_to_ndcg_at_100_map[le]))\n",
        "        Y.append(ans * 100.0)\n",
        "    \n",
        "    # Smoothening\n",
        "    Y_mine = []\n",
        "    prev_5 = []\n",
        "    for i in Y:\n",
        "        prev_5.append(i)\n",
        "        if len(prev_5) > 5: del prev_5[0]\n",
        "\n",
        "        temp = 0.0\n",
        "        for j in prev_5: temp += float(j)\n",
        "        temp = float(temp) / float(len(prev_5))\n",
        "        Y_mine.append(temp)\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(X, Y_mine, label='SVAE')\n",
        "    plt.xlabel(\"Number of items in the fold-out set\")\n",
        "    plt.ylabel(\"Average NDCG@100\")\n",
        "    plt.title(hyper_params['project_name'])\n",
        "    if not os.path.isdir(\"saved_plots/\"): os.mkdir(\"saved_plots/\")\n",
        "    plt.savefig(\"saved_plots/seq_len_vs_ndcg_\" + hyper_params['project_name'] + \".pdf\")\n",
        "\n",
        "    leg = plt.legend(loc='best', ncol=2)\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogv3LSUm6Ejy",
        "colab_type": "text"
      },
      "source": [
        "# Data Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FA1mzNPj6Ejz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(hyper_params):\n",
        "    \n",
        "    file_write(hyper_params['log_file'], \"Started reading data file\")\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'train.csv')\n",
        "    lines_train = f.readlines()[1:]\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'validation_tr.csv')\n",
        "    lines_val_tr = f.readlines()[1:]\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'validation_te.csv')\n",
        "    lines_val_te = f.readlines()[1:]\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'test_tr.csv')\n",
        "    lines_test_tr = f.readlines()[1:]\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'test_te.csv')\n",
        "    lines_test_te = f.readlines()[1:]\n",
        "    \n",
        "    unique_sid = list()\n",
        "    with open(hyper_params['data_base'] + 'unique_sid.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "    num_items = len(unique_sid)\n",
        "    \n",
        "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
        "\n",
        "    train_reader = DataReader(hyper_params, lines_train, None, num_items, True)\n",
        "    val_reader = DataReader(hyper_params, lines_val_tr, lines_val_te, num_items, False)\n",
        "    test_reader = DataReader(hyper_params, lines_test_tr, lines_test_te, num_items, False)\n",
        "\n",
        "    return train_reader, val_reader, test_reader, num_items\n",
        "\n",
        "class DataReader:\n",
        "    def __init__(self, hyper_params, a, b, num_items, is_training):\n",
        "        self.hyper_params = hyper_params\n",
        "        self.batch_size = hyper_params['batch_size']\n",
        "        \n",
        "        num_users = 0\n",
        "        min_user = 1000000000000000000000000 # Infinity\n",
        "        for line in a:\n",
        "            line = line.strip().split(\",\")\n",
        "            num_users = max(num_users, int(line[0]))\n",
        "            min_user = min(min_user, int(line[0]))\n",
        "        num_users = num_users - min_user + 1\n",
        "        \n",
        "        self.num_users = num_users\n",
        "        self.min_user = min_user\n",
        "        self.num_items = num_items\n",
        "        \n",
        "        self.data_train = a\n",
        "        self.data_test = b\n",
        "        self.is_training = is_training\n",
        "        self.all_users = []\n",
        "        \n",
        "        self.prep()\n",
        "        self.number()\n",
        "\n",
        "    def prep(self):\n",
        "        self.data = []\n",
        "        for i in range(self.num_users): self.data.append([])\n",
        "            \n",
        "        for i in tqdm(range(len(self.data_train))):\n",
        "            line = self.data_train[i]\n",
        "            line = line.strip().split(\",\")\n",
        "            self.data[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
        "        \n",
        "        if self.is_training == False:\n",
        "            self.data_te = []\n",
        "            for i in range(self.num_users): self.data_te.append([])\n",
        "                \n",
        "            for i in tqdm(range(len(self.data_test))):\n",
        "                line = self.data_test[i]\n",
        "                line = line.strip().split(\",\")\n",
        "                self.data_te[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
        "                \n",
        "        \n",
        "    def number(self):\n",
        "        self.num_b = int(min(len(self.data), self.hyper_params['number_users_to_keep']) / self.batch_size)\n",
        "    \n",
        "    def iter(self):\n",
        "        users_done = 0\n",
        "\n",
        "        x_batch = []\n",
        "        \n",
        "        user_iterate_order = list(range(len(self.data)))\n",
        "        \n",
        "        # Randomly shuffle the training order\n",
        "        np.random.shuffle(user_iterate_order)\n",
        "        \n",
        "        for user in user_iterate_order:\n",
        "\n",
        "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
        "            users_done += 1\n",
        "            \n",
        "            y_batch_s = torch.zeros(self.batch_size, len(self.data[user]) - 1, self.num_items)\n",
        "            if is_cuda_available: y_batch_s = y_batch_s.cuda()\n",
        "            \n",
        "            if self.hyper_params['loss_type'] == 'predict_next':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in [ self.data[user][timestep + 1] ] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'next_k':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in self.data[user][timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'postfix':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in self.data[user][timestep + 1:] ]), 1.0\n",
        "                    )\n",
        "            \n",
        "            x_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
        "            \n",
        "            if len(x_batch) == self.batch_size: # batch_size always = 1\n",
        "            \n",
        "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False)\n",
        "                x_batch = []\n",
        "\n",
        "    def iter_eval(self):\n",
        "\n",
        "        x_batch = []\n",
        "        test_movies, test_movies_r = [], []\n",
        "        \n",
        "        users_done = 0\n",
        "        \n",
        "        for user in range(len(self.data)):\n",
        "            \n",
        "            users_done += 1\n",
        "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
        "            \n",
        "            if self.is_training == True: \n",
        "                split = float(self.hyper_params['history_split_test'][0])\n",
        "                base_predictions_on = self.data[user][:int(split * len(self.data[user]))]\n",
        "                heldout_movies = self.data[user][int(split * len(self.data[user])):]\n",
        "            else:\n",
        "                base_predictions_on = self.data[user]\n",
        "                heldout_movies = self.data_te[user]\n",
        "            \n",
        "            y_batch_s = torch.zeros(self.batch_size, len(base_predictions_on) - 1, self.num_items).cuda()\n",
        "            \n",
        "            if self.hyper_params['loss_type'] == 'predict_next':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in [ base_predictions_on[timestep + 1] ] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'next_k':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in base_predictions_on[timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'postfix':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in base_predictions_on[timestep + 1:] ]), 1.0\n",
        "                    )\n",
        "            \n",
        "            test_movies.append([ i[0] for i in heldout_movies ])\n",
        "            test_movies_r.append([ i[1] for i in heldout_movies ])\n",
        "            x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
        "            \n",
        "            if len(x_batch) == self.batch_size: # batch_size always = 1\n",
        "                \n",
        "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False), test_movies, test_movies_r\n",
        "                x_batch = []\n",
        "                test_movies, test_movies_r = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnh1jY9De10L",
        "colab_type": "text"
      },
      "source": [
        "# Data Load 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4BsH4k-e1F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data2(hyper_params):\n",
        "      \n",
        "    f = open(hyper_params['data_base'] + 'test_tr.csv')\n",
        "    lines_test_tr = f.readlines()[1:]\n",
        "    \n",
        "    f = open(hyper_params['data_base'] + 'test_te.csv')\n",
        "    lines_test_te = f.readlines()[1:]\n",
        "    \n",
        "    unique_sid = list()\n",
        "    with open(hyper_params['data_base'] + 'unique_sid.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "    num_items = len(unique_sid)\n",
        "    \n",
        "    file_write(hyper_params['log_file'], \"Data Files loaded!\")\n",
        "\n",
        "    test_reader = DataReader2(hyper_params, lines_test_tr, lines_test_te, num_items, False)\n",
        "\n",
        "    return test_reader, num_items\n",
        "    \n",
        "class DataReader2:\n",
        "    def __init__(self, hyper_params, a, b, num_items, is_training):\n",
        "        self.hyper_params = hyper_params\n",
        "        self.batch_size = hyper_params['batch_size']\n",
        "        \n",
        "        num_users = 0\n",
        "        min_user = 1000000000000000000000000 # Infinity\n",
        "        for line in a:\n",
        "            line = line.strip().split(\",\")\n",
        "            num_users = max(num_users, int(line[0]))\n",
        "            min_user = min(min_user, int(line[0]))\n",
        "        num_users = num_users - min_user + 1\n",
        "        \n",
        "        self.num_users = num_users\n",
        "        self.min_user = min_user\n",
        "        self.num_items = num_items\n",
        "        \n",
        "        self.data_train = a\n",
        "        self.data_test = b\n",
        "        self.is_training = is_training\n",
        "        self.all_users = []\n",
        "        \n",
        "        self.prep()\n",
        "        self.number()\n",
        "\n",
        "    def prep(self):\n",
        "        self.data = []\n",
        "        for i in range(self.num_users): self.data.append([])\n",
        "            \n",
        "        for i in tqdm(range(len(self.data_train))):\n",
        "            line = self.data_train[i]\n",
        "            line = line.strip().split(\",\")\n",
        "            self.data[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
        "        \n",
        "        if self.is_training == False:\n",
        "            self.data_te = []\n",
        "            for i in range(self.num_users): self.data_te.append([])\n",
        "                \n",
        "            for i in tqdm(range(len(self.data_test))):\n",
        "                line = self.data_test[i]\n",
        "                line = line.strip().split(\",\")\n",
        "                self.data_te[int(line[0]) - self.min_user].append([ int(line[1]), 1 ])\n",
        "                \n",
        "        \n",
        "    def number(self):\n",
        "        self.num_b = int(min(len(self.data), self.hyper_params['number_users_to_keep']) / self.batch_size)\n",
        "    \n",
        "    def iter(self):\n",
        "        users_done = 0\n",
        "\n",
        "        x_batch = []\n",
        "        \n",
        "        user_iterate_order = list(range(len(self.data)))\n",
        "        \n",
        "        # Randomly shuffle the training order\n",
        "        np.random.shuffle(user_iterate_order)\n",
        "        \n",
        "        for user in user_iterate_order:\n",
        "\n",
        "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
        "            users_done += 1\n",
        "            \n",
        "            y_batch_s = torch.zeros(self.batch_size, len(self.data[user]) - 1, self.num_items)\n",
        "            if is_cuda_available: y_batch_s = y_batch_s.cuda()\n",
        "            \n",
        "            if self.hyper_params['loss_type'] == 'predict_next':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in [ self.data[user][timestep + 1] ] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'next_k':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in self.data[user][timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'postfix':\n",
        "                for timestep in range(len(self.data[user]) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in self.data[user][timestep + 1:] ]), 1.0\n",
        "                    )\n",
        "            \n",
        "            x_batch.append([ i[0] for i in self.data[user][:-1] ])\n",
        "            \n",
        "            if len(x_batch) == self.batch_size: # batch_size always = 1\n",
        "            \n",
        "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False)\n",
        "                x_batch = []\n",
        "\n",
        "    def iter_eval(self):\n",
        "\n",
        "        x_batch = []\n",
        "        test_movies, test_movies_r = [], []\n",
        "        \n",
        "        users_done = 0\n",
        "        \n",
        "        for user in range(len(self.data)):\n",
        "            \n",
        "            users_done += 1\n",
        "            if users_done > self.hyper_params['number_users_to_keep']: break\n",
        "            \n",
        "            if self.is_training == True: \n",
        "                split = float(self.hyper_params['history_split_test'][0])\n",
        "                base_predictions_on = self.data[user][:int(split * len(self.data[user]))]\n",
        "                heldout_movies = self.data[user][int(split * len(self.data[user])):]\n",
        "            else:\n",
        "                base_predictions_on = self.data[user]\n",
        "                heldout_movies = self.data_te[user]\n",
        "            \n",
        "            y_batch_s = torch.zeros(self.batch_size, len(base_predictions_on) - 1, self.num_items).cuda()\n",
        "            \n",
        "            if self.hyper_params['loss_type'] == 'predict_next':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in [ base_predictions_on[timestep + 1] ] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'next_k':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in base_predictions_on[timestep + 1:][:self.hyper_params['next_k']] ]), 1.0\n",
        "                    )\n",
        "                \n",
        "            elif self.hyper_params['loss_type'] == 'postfix':\n",
        "                for timestep in range(len(base_predictions_on) - 1):\n",
        "                    y_batch_s[len(x_batch), timestep, :].scatter_(\n",
        "                        0, LongTensor([ i[0] for i in base_predictions_on[timestep + 1:] ]), 1.0\n",
        "                    )\n",
        "            \n",
        "            test_movies.append([ i[0] for i in heldout_movies ])\n",
        "            test_movies_r.append([ i[1] for i in heldout_movies ])\n",
        "            x_batch.append([ i[0] for i in base_predictions_on[:-1] ])\n",
        "            \n",
        "            if len(x_batch) == self.batch_size: # batch_size always = 1\n",
        "                \n",
        "                yield Variable(LongTensor(x_batch)), Variable(y_batch_s, requires_grad=False), test_movies, test_movies_r\n",
        "                x_batch = []\n",
        "                test_movies, test_movies_r = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbwU0mfu6Ej2",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CB0s1ZSD6Ej2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion, reader, hyper_params, is_train_set):\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['loss'] = 0.0\n",
        "    Ks = [10, 100]\n",
        "    for k in Ks: \n",
        "        metrics['NDCG@' + str(k)] = 0.0\n",
        "        metrics['Rec@' + str(k)] = 0.0\n",
        "        metrics['Prec@' + str(k)] = 0.0\n",
        "\n",
        "    batch = 0.0\n",
        "    total_users = 0.0\n",
        "    #print (total_users)\n",
        "    # For plotting the results (seq length vs. NDCG@100)\n",
        "    len_to_ndcg_at_100_map = {}\n",
        "\n",
        "    for x, y_s, test_movies, test_movies_r in reader.iter_eval():\n",
        "        #print(x, y_s, test_movies, test_movies_r)\n",
        "        batch += 1\n",
        "        if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
        "        #if is_train_set == False:\n",
        "          #hyper_params['noise']= noise\n",
        "        decoder_output, z_mean, z_log_sigma = model(x)\n",
        "        #print('output decoder')\n",
        "        #print(decoder_output.shape)\n",
        "        metrics['loss'] += criterion(decoder_output, z_mean, z_log_sigma, y_s, 0.2).data\n",
        "        \n",
        "        # Making the logits of previous items in the sequence to be \"- infinity\"\n",
        "        decoder_output = decoder_output.data\n",
        "        #print('after decoder after criterian')\n",
        "        #print(decoder_output.shape)\n",
        "        x_scattered = torch.zeros(decoder_output.shape[0], decoder_output.shape[2])\n",
        "        #print('x_scattered')\n",
        "        #print(x_scattered.shape)\n",
        "        if is_cuda_available: x_scattered = x_scattered.cuda()\n",
        "        #print('x scattered after ')\n",
        "        x_scattered[0, :].scatter_(0, x[0].data, 1.0)\n",
        "\n",
        "        last_predictions = decoder_output[:, -1, :] - (torch.abs(decoder_output[:, -1, :] * x_scattered) * 100000000)\n",
        "        #print('last pred')\n",
        "        #print(last_predictions.shape)\n",
        "        for batch_num in range(last_predictions.shape[0]): # batch_num is ideally only 0, since batch_size is enforced to be always 1\n",
        "            predicted_scores = last_predictions[batch_num]\n",
        "            #print('score')\n",
        "            #print(predicted_scores.shape)\n",
        "\n",
        "            actual_movies_watched = test_movies[batch_num]\n",
        "            actual_movies_ratings = test_movies_r[batch_num]\n",
        "                    \n",
        "            # Calculate NDCG\n",
        "            _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
        "            for k in Ks:\n",
        "                best, now_at, dcg, hits = 0.0, 0.0, 0.0, 0.0\n",
        "                \n",
        "                rec_list = list(argsorted[:k].cpu().numpy())\n",
        "                for m in range(len(actual_movies_watched)):\n",
        "                    movie = actual_movies_watched[m]\n",
        "                    now_at += 1.0\n",
        "                    if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
        "                    \n",
        "                    if movie not in rec_list: continue\n",
        "                    hits += 1.0\n",
        "                    dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
        "                \n",
        "                metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
        "                metrics['Rec@' + str(k)] += float(hits) / float(len(actual_movies_watched))\n",
        "                metrics['Prec@' + str(k)] += float(hits) / float(k)\n",
        "                \n",
        "                # Only for plotting the graph (seq length vs. NDCG@100)\n",
        "                if k == 100:\n",
        "                    seq_len = int(len(actual_movies_watched)) + int(x[batch_num].shape[0]) + 1\n",
        "                    if seq_len not in len_to_ndcg_at_100_map: len_to_ndcg_at_100_map[seq_len] = []\n",
        "                    len_to_ndcg_at_100_map[seq_len].append(float(dcg) / float(best))\n",
        "                \n",
        "            total_users += 1.0\n",
        "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
        "    metrics['loss'] = round(metrics['loss'], 4)\n",
        "    \n",
        "    for k in Ks:\n",
        "        metrics['NDCG@' + str(k)] = round((100.0 * metrics['NDCG@' + str(k)]) / float(total_users), 4)\n",
        "        metrics['Rec@' + str(k)] = round((100.0 * metrics['Rec@' + str(k)]) / float(total_users), 4)\n",
        "        metrics['Prec@' + str(k)] = round((100.0 * metrics['Prec@' + str(k)]) / float(total_users), 4)\n",
        "        \n",
        "    return metrics, len_to_ndcg_at_100_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzpbIMYCK65D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8gmmoxbNotf",
        "colab_type": "text"
      },
      "source": [
        "# Selecting users for dcg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZwpxb1v34t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate1(model, criterion, reader, hyper_params, is_train_set):\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['loss'] = 0.0\n",
        "    Ks = [10, 100]\n",
        "    for k in Ks: \n",
        "        metrics['NDCG@' + str(k)] = 0.0\n",
        "        metrics['Rec@' + str(k)] = 0.0\n",
        "        metrics['Prec@' + str(k)] = 0.0\n",
        "\n",
        "    batch = 0\n",
        "    total_users = 0.0\n",
        "    \n",
        "    # For plotting the results (seq length vs. NDCG@100)\n",
        "    len_to_ndcg_at_100_map = {}\n",
        "    #for rounds in num_rounds:\n",
        "    decoder_output_arr = []\n",
        "    decoder_output_arr2 = np.empty((0,20108) ,float)\n",
        "    # print(decoder_output_arr) \n",
        "    for x, y_s, test_movies, test_movies_r in reader.iter_eval():\n",
        "\n",
        "        \n",
        "        \n",
        "        #print(x, y_s, test_movies, test_movies_r)\n",
        "        \n",
        "        #print('batch')\n",
        "        #print(batch)\n",
        "        if batch == 300:\n",
        "          break \n",
        "\n",
        "        else:          \n",
        "          batch += 1\n",
        "\n",
        "          if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
        "\n",
        "     \n",
        "          decoder_output, z_mean, z_log_sigma = model(x)\n",
        "          \n",
        "          # print(decoder_output.shape)\n",
        "          # print(decoder_output)\n",
        "\n",
        "        \n",
        "          metrics['loss'] += criterion(decoder_output, z_mean, z_log_sigma, y_s, 0.2).data\n",
        "          \n",
        "          # Making the logits of previous items in the sequence to be \"- infinity\"\n",
        "          decoder_output = decoder_output.data\n",
        "          # test = decoder_output[:, -1, :]\n",
        "          # print('test')\n",
        "          # print(test.shape)\n",
        "          \n",
        "          # print(test)\n",
        "      \n",
        "          \n",
        "          #print('after decoder after criterian')\n",
        "          #print(decoder_output.shape)\n",
        "          x_scattered = torch.zeros(decoder_output.shape[0], decoder_output.shape[2])\n",
        "          #print('x_scattered')\n",
        "        # print(x_scattered.shape)\n",
        "        # print(x_scattered)\n",
        "          if is_cuda_available: x_scattered = x_scattered.cuda()\n",
        "          #print('x scattered after ')\n",
        "          #print(x_scattered)\n",
        "          x_scattered[0, :].scatter_(0, x[0].data, 1.0)\n",
        "\n",
        "          last_predictions = decoder_output[:, -1, :] - (torch.abs(decoder_output[:, -1, :] * x_scattered) * 100000000)\n",
        "         \n",
        "          decoder_output_arr.append(last_predictions.cpu().numpy())\n",
        "          decoder_output_arr2 = np.vstack((decoder_output_arr2, last_predictions.cpu().numpy()))\n",
        "         \n",
        "          for batch_num in range(last_predictions.shape[0]): # batch_num is ideally only 0, since batch_size is enforced to be always 1\n",
        "              predicted_scores = last_predictions[batch_num]\n",
        "           \n",
        "\n",
        "              actual_movies_watched = test_movies[batch_num]\n",
        "              actual_movies_ratings = [batch_num]\n",
        "        \n",
        "              # Calculate NDCGtest_movies_r\n",
        "              _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
        "              # print('arg')\n",
        "              # print(argsorted)\n",
        "              # print(argsorted.shape)\n",
        "              for k in Ks:\n",
        "                  best, now_at, dcg, hits = 0.0, 0.0, 0.0, 0.0\n",
        "                  \n",
        "                  rec_list = list(argsorted[:k].cpu().numpy())\n",
        "                  for m in range(len(actual_movies_watched)):\n",
        "                      movie = actual_movies_watched[m]\n",
        "                      now_at += 1.0\n",
        "                      if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
        "                      \n",
        "                      if movie not in rec_list: continue\n",
        "                      hits += 1.0\n",
        "                      dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
        "                  \n",
        "                  metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
        "                  metrics['Rec@' + str(k)] += float(hits) / float(len(actual_movies_watched))\n",
        "                  metrics['Prec@' + str(k)] += float(hits) / float(k)\n",
        "                  \n",
        "                  # Only for plotting the graph (seq length vs. NDCG@100)\n",
        "                  if k == 100:\n",
        "                      seq_len = int(len(actual_movies_watched)) + int(x[batch_num].shape[0]) + 1\n",
        "                      if seq_len not in len_to_ndcg_at_100_map: len_to_ndcg_at_100_map[seq_len] = []\n",
        "                      len_to_ndcg_at_100_map[seq_len].append(float(dcg) / float(best))\n",
        "                  \n",
        "              total_users += 1.0\n",
        "           \n",
        "\n",
        "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
        "    metrics['loss'] = round(metrics['loss'], 4)\n",
        "    \n",
        "\n",
        "    return metrics, len_to_ndcg_at_100_map,decoder_output_arr,decoder_output_arr2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZkdP_oNMny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYeXLAYhNe9d",
        "colab_type": "text"
      },
      "source": [
        "# Fairness Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEHMLz9ZcvTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate2(model, criterion, reader, hyper_params, is_train_set, num_rounds):\n",
        "    model.eval()\n",
        "    metrics = {}\n",
        "    metrics['loss'] = 0.0\n",
        "    Ks = [10, 100]\n",
        "    for k in Ks: \n",
        "        metrics['NDCG@' + str(k)] = 0.0\n",
        "        metrics['Rec@' + str(k)] = 0.0\n",
        "        metrics['Prec@' + str(k)] = 0.0\n",
        "\n",
        "    \n",
        "    \n",
        "    # For plotting the results (seq length vs. NDCG@100)\n",
        "    \n",
        "\n",
        "    decoder_output_list = []\n",
        "    for rounds in range(0,num_rounds):\n",
        "      len_to_ndcg_at_100_map = {}\n",
        "      total_users = 0.0\n",
        "      decoder_output_arr = []\n",
        "      batch = 0\n",
        "      decoder_output_arr2 = np.empty((0,20108) ,float)\n",
        "      for x, y_s, test_movies, test_movies_r in reader.iter_eval():\n",
        "          if batch == 300:\n",
        "            break \n",
        "\n",
        "          else:          \n",
        "            batch += 1\n",
        "\n",
        "            if is_train_set == True and batch > hyper_params['train_cp_users']: break\n",
        "            decoder_output, z_mean, z_log_sigma = model(x)\n",
        "            \n",
        "          \n",
        "            metrics['loss'] += criterion(decoder_output, z_mean, z_log_sigma, y_s, 0.2).data\n",
        "            \n",
        "            # Making the logits of previous items in the sequence to be \"- infinity\"\n",
        "            decoder_output = decoder_output.data\n",
        "            \n",
        "            x_scattered = torch.zeros(decoder_output.shape[0], decoder_output.shape[2])\n",
        "            \n",
        "            if is_cuda_available: x_scattered = x_scattered.cuda()\n",
        "            \n",
        "            x_scattered[0, :].scatter_(0, x[0].data, 1.0)\n",
        "\n",
        "            last_predictions = decoder_output[:, -1, :] - (torch.abs(decoder_output[:, -1, :] * x_scattered) * 100000000)\n",
        "            \n",
        "            decoder_output_arr.append(last_predictions.cpu().numpy())\n",
        "            decoder_output_arr2 = np.vstack((decoder_output_arr2, last_predictions.cpu().numpy()))            \n",
        "         \n",
        "            for batch_num in range(last_predictions.shape[0]): # batch_num is ideally only 0, since batch_size is enforced to be always 1\n",
        "                predicted_scores = last_predictions[batch_num]\n",
        "\n",
        "                actual_movies_watched = test_movies[batch_num]\n",
        "                actual_movies_ratings = [batch_num]\n",
        "          \n",
        "                # Calculate NDCGtest_movies_r\n",
        "                _, argsorted = torch.sort(-1.0 * predicted_scores)\n",
        "\n",
        "                for k in Ks:\n",
        "                    best, now_at, dcg, hits = 0.0, 0.0, 0.0, 0.0\n",
        "                    \n",
        "                    rec_list = list(argsorted[:k].cpu().numpy())\n",
        "                    for m in range(len(actual_movies_watched)):\n",
        "                        movie = actual_movies_watched[m]\n",
        "                        now_at += 1.0\n",
        "                        if now_at <= k: best += 1.0 / float(np.log2(now_at + 1))\n",
        "                        \n",
        "                        if movie not in rec_list: continue\n",
        "                        hits += 1.0\n",
        "                        dcg += 1.0 / float(np.log2(float(rec_list.index(movie) + 2)))\n",
        "                    \n",
        "                    metrics['NDCG@' + str(k)] += float(dcg) / float(best)\n",
        "                    metrics['Rec@' + str(k)] += float(hits) / float(len(actual_movies_watched))\n",
        "                    metrics['Prec@' + str(k)] += float(hits) / float(k)\n",
        "                    \n",
        "                    # Only for plotting the graph (seq length vs. NDCG@100)\n",
        "                    if k == 100:\n",
        "                        seq_len = int(len(actual_movies_watched)) + int(x[batch_num].shape[0]) + 1\n",
        "                        if seq_len not in len_to_ndcg_at_100_map: len_to_ndcg_at_100_map[seq_len] = []\n",
        "                        len_to_ndcg_at_100_map[seq_len].append(float(dcg) / float(best))\n",
        "                    \n",
        "                total_users += 1.0\n",
        "                \n",
        "      decoder_output_list.append(decoder_output_arr2)\n",
        "\n",
        "\n",
        "    metrics['loss'] = float(metrics['loss']) / float(batch)\n",
        "    metrics['loss'] = round(metrics['loss'], 4)\n",
        "    \n",
        "  \n",
        "    return metrics, len_to_ndcg_at_100_map,decoder_output_arr,decoder_output_arr2,decoder_output_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1IiY6WR6Ej5",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDr7l0KUmbV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_noise = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yzBSqNQ96Ej6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hyper_params):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(\n",
        "            hyper_params['rnn_size'], hyper_params['hidden_size']\n",
        "        )\n",
        "        nn.init.xavier_normal(self.linear1.weight)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hyper_params):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(hyper_params['latent_size'], hyper_params['hidden_size'])\n",
        "        self.linear2 = nn.Linear(hyper_params['hidden_size'], hyper_params['total_items'])\n",
        "        nn.init.xavier_normal(self.linear1.weight)\n",
        "        nn.init.xavier_normal(self.linear2.weight)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hyper_params):\n",
        "        super(Model, self).__init__()\n",
        "        self.hyper_params = hyper_params\n",
        "        # self.noise = hyper_params['noise'] \n",
        "        self.encoder = Encoder(hyper_params)\n",
        "        self.decoder = Decoder(hyper_params)\n",
        "        \n",
        "        # Since we don't need padding, our vocabulary size = \"hyper_params['total_items']\" and not \"hyper_params['total_items'] + 1\"\n",
        "        self.item_embed = nn.Embedding(hyper_params['total_items'], hyper_params['item_embed_size'])\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            hyper_params['item_embed_size'], hyper_params['rnn_size'], \n",
        "            batch_first = True, num_layers = 1\n",
        "        )\n",
        "        \n",
        "        self.linear1 = nn.Linear(hyper_params['hidden_size'], 2 * hyper_params['latent_size'])\n",
        "        nn.init.xavier_normal(self.linear1.weight)\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "    def add_noise(self, noise, sigma):\n",
        "        # print(\"Input Noise: \", noise)\n",
        "        return {\n",
        "           1: torch.from_numpy(np.random.normal(0, 0.5, size=sigma.size())).float(),\n",
        "           2: torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float(),\n",
        "           3: torch.from_numpy(np.random.normal(0, 2, size=sigma.size())).float(),\n",
        "           4: torch.from_numpy(np.random.uniform(-1, 1, size=sigma.size())).float(),\n",
        "        }.get(noise, torch.zeros(1))\n",
        "          \n",
        "    def sample_latent(self, h_enc):\n",
        "        \"\"\"\n",
        "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
        "        \"\"\"\n",
        "        temp_out = self.linear1(h_enc)\n",
        "        \n",
        "        mu = temp_out[:, :self.hyper_params['latent_size']]\n",
        "        log_sigma = temp_out[:, self.hyper_params['latent_size']:]\n",
        "        \n",
        "        sigma = torch.exp(log_sigma)\n",
        "#         std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
        "#         if is_cuda_available: std_z = std_z.cuda()\n",
        "\n",
        "        self.z_mean = mu\n",
        "        self.z_log_sigma = log_sigma\n",
        "        \n",
        "        #noise = self.add_noise(2, sigma)\n",
        "        #if is_cuda_available: noise = noise.cuda()  \n",
        "        std_z = self.add_noise(s_noise, sigma)  \n",
        "        # std_z = self.add_noise(self.noise, sigma)        \n",
        "        # std_z = self.add_noise(self.hyper_params['noise'], sigma)\n",
        "        if is_cuda_available: std_z = std_z.cuda()\n",
        "        return mu + sigma * Variable(std_z, requires_grad=False)\n",
        "        #return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "#         return mu + noise * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "#         return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_shape = x.shape                                      # [bsz x seq_len] = [1 x seq_len]\n",
        "        x = x.view(-1)                                          # [seq_len]\n",
        "        \n",
        "        x = self.item_embed(x)                                  # [seq_len x embed_size]\n",
        "        x = x.view(in_shape[0], in_shape[1], -1)                # [1 x seq_len x embed_size]\n",
        "        \n",
        "        rnn_out, _ = self.gru(x)                                # [1 x seq_len x rnn_size]\n",
        "        rnn_out = rnn_out.view(in_shape[0] * in_shape[1], -1)   # [seq_len x rnn_size]\n",
        "        \n",
        "        enc_out = self.encoder(rnn_out)                         # [seq_len x hidden_size]\n",
        "        sampled_z = self.sample_latent(enc_out)                 # [seq_len x latent_size]\n",
        "        \n",
        "        dec_out = self.decoder(sampled_z)                       # [seq_len x total_items]\n",
        "        dec_out = dec_out.view(in_shape[0], in_shape[1], -1)    # [1 x seq_len x total_items]\n",
        "                              \n",
        "        return dec_out, self.z_mean, self.z_log_sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tmTDgAo6Ej8",
        "colab_type": "text"
      },
      "source": [
        "# Custom loss\n",
        "\n",
        "$$ Loss \\; = \\; \\sum_{u \\in U} Loss_u $$ <br>\n",
        "$$ Loss_u \\; = \\; \\beta * KL( \\, \\phi(z \\vert x) \\, \\Vert \\, {\\rm I\\!N(0, I)} \\, ) \\; - \\; log( \\, P_{\\phi}(g_{\\theta}(x)) \\, ) $$ <br>\n",
        "$ g_{\\theta}(.)$ is the encoder ; $P_{\\phi}(.)$ is the decoded distribution; $ \\beta $ is the anneal factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jH_iYZJ36Ej9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAELoss(torch.nn.Module):\n",
        "    def __init__(self, hyper_params):\n",
        "        super(VAELoss,self).__init__()\n",
        "        self.hyper_params = hyper_params\n",
        "\n",
        "    def forward(self, decoder_output, mu_q, logvar_q, y_true_s, anneal):\n",
        "        # Calculate KL Divergence loss\n",
        "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
        "    \n",
        "        # Calculate Likelihood\n",
        "        dec_shape = decoder_output.shape # [batch_size x seq_len x total_items] = [1 x seq_len x total_items]\n",
        "\n",
        "        decoder_output = F.log_softmax(decoder_output, -1)\n",
        "        num_ones = float(torch.sum(y_true_s[0, 0]))\n",
        "        \n",
        "        likelihood = torch.sum(\n",
        "            -1.0 * y_true_s.view(dec_shape[0] * dec_shape[1], -1) * \\\n",
        "            decoder_output.view(dec_shape[0] * dec_shape[1], -1)\n",
        "        ) / (float(self.hyper_params['batch_size']) * num_ones)\n",
        "        \n",
        "        final = (anneal * kld) + (likelihood)\n",
        "        \n",
        "        return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaXGABqi0F_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWcTU_f0JQd",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVeZEzzB0PRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(reader):\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    batch = 0\n",
        "    batch_limit = int(train_reader.num_b)\n",
        "    total_anneal_steps = 200000\n",
        "    anneal = 0.0\n",
        "    update_count = 0.0\n",
        "    anneal_cap = 0.2\n",
        "    \n",
        "    for x, y_s in reader.iter():\n",
        "        \n",
        "        batch += 1\n",
        "        \n",
        "        # Empty the gradients\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "        # Forward pass\n",
        "        decoder_output, z_mean, z_log_sigma = model(x)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss = criterion(decoder_output, z_mean, z_log_sigma, y_s, anneal)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "        \n",
        "        # Anneal logic\n",
        "        if total_anneal_steps > 0:\n",
        "            anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
        "        else:\n",
        "            anneal = anneal_cap\n",
        "        update_count += 1.0\n",
        "        \n",
        "        # Logging mechanism\n",
        "        if (batch % hyper_params['batch_log_interval'] == 0 and batch > 0) or batch == batch_limit:\n",
        "            div = hyper_params['batch_log_interval']\n",
        "            if batch == batch_limit: div = (batch_limit % hyper_params['batch_log_interval']) - 1\n",
        "            if div <= 0: div = 1\n",
        "\n",
        "            cur_loss = (total_loss / div)\n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            ss = '| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f}'.format(\n",
        "                    epoch, batch, batch_limit, (elapsed * 1000) / div, cur_loss\n",
        "            )\n",
        "            \n",
        "            file_write(hyper_params['log_file'], ss)\n",
        "\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "# Train It..\n",
        "train_reader, val_reader, test_reader, total_items = load_data(hyper_params)\n",
        "hyper_params['total_items'] = total_items\n",
        "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
        "\n",
        "file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
        "file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
        "file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(train_reader.num_b))\n",
        "file_write(hyper_params['log_file'], \"Number of validation batches: {:4d}\".format(val_reader.num_b))\n",
        "file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(test_reader.num_b))\n",
        "file_write(hyper_params['log_file'], \"Total Items: \" + str(total_items) + \"\\n\")\n",
        "\n",
        "model = Model(hyper_params)\n",
        "if is_cuda_available: model.cuda()\n",
        "\n",
        "criterion = VAELoss(hyper_params)\n",
        "\n",
        "if hyper_params['optimizer'] == 'adagrad':\n",
        "    optimizer = torch.optim.Adagrad(\n",
        "        model.parameters(), weight_decay=hyper_params['weight_decay'], lr = hyper_params['learning_rate']\n",
        "    )\n",
        "elif hyper_params['optimizer'] == 'adadelta':\n",
        "    optimizer = torch.optim.Adadelta(\n",
        "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
        "    )\n",
        "elif hyper_params['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
        "    )\n",
        "elif hyper_params['optimizer'] == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(\n",
        "        model.parameters(), weight_decay=hyper_params['weight_decay']\n",
        "    )\n",
        "\n",
        "file_write(hyper_params['log_file'], str(model))\n",
        "file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
        "\n",
        "best_val_ndcg = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, hyper_params['epochs'] + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        \n",
        "        train(train_reader)\n",
        "        \n",
        "        # Calulating the metrics on the train set\n",
        "        metrics, _ = evaluate(model, criterion, train_reader, hyper_params, True)\n",
        "        string = \"\"\n",
        "        for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
        "        string += ' (TRAIN)'\n",
        "    \n",
        "        # Calulating the metrics on the validation set\n",
        "        metrics, _ = evaluate(model, criterion, val_reader, hyper_params, False)\n",
        "        string2 = \"\"\n",
        "        for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
        "        string2 += ' (VAL)'\n",
        "\n",
        "        ss  = '-' * 89\n",
        "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
        "        ss += string\n",
        "        ss += '\\n'\n",
        "        ss += '-' * 89\n",
        "        ss += '\\n| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time))\n",
        "        ss += string2\n",
        "        ss += '\\n'\n",
        "        ss += '-' * 89\n",
        "        file_write(hyper_params['log_file'], ss)\n",
        "        \n",
        "        if not best_val_ndcg or metrics['NDCG@100'] >= best_val_ndcg:\n",
        "            with open(hyper_params['model_file_name'], 'wb') as f: torch.save(model, f)\n",
        "            best_val_ndcg = metrics['NDCG@100']\n",
        "\n",
        "except KeyboardInterrupt: print('Exiting from training early')\n",
        "\n",
        "# Plot Traning graph\n",
        "f = open(model.hyper_params['log_file'])\n",
        "lines = f.readlines()\n",
        "lines.reverse()\n",
        "\n",
        "train = []\n",
        "test = []\n",
        "\n",
        "for line in lines:\n",
        "    if line[:10] == 'Simulation' and len(train) > 1: break\n",
        "    elif line[:10] == 'Simulation' and len(train) <= 1: train, test = [], []\n",
        "        \n",
        "    if line[2:5] == 'end' and line[-5:-2] == 'VAL': test.append(line.strip().split(\"|\"))\n",
        "    elif line[2:5] == 'end' and line[-7:-2] == 'TRAIN': train.append(line.strip().split(\"|\"))\n",
        "\n",
        "train.reverse()\n",
        "test.reverse()\n",
        "\n",
        "train_ndcg = []\n",
        "test_ndcg = []\n",
        "test_loss, train_loss = [], []\n",
        "\n",
        "for i in train:\n",
        "    for metric in i:\n",
        "        if metric.split(\"=\")[0] == \" NDCG@100 \":\n",
        "            train_ndcg.append(float(metric.split('=')[1].split(' ')[1]))\n",
        "        if metric.split(\"=\")[0] == \" loss \":\n",
        "            train_loss.append(float(metric.split(\"=\")[1].split(' ')[1]))\n",
        "\n",
        "total, avg_runtime = 0.0, 0.0\n",
        "for i in test:\n",
        "    avg_runtime += float(i[2].split(\" \")[2][:-1])\n",
        "    total += 1.0\n",
        "    \n",
        "    for metric in i:\n",
        "        if metric.split(\"=\")[0] == \" NDCG@100 \":\n",
        "            test_ndcg.append(float(metric.split('=')[1].split(' ')[1]))\n",
        "        if metric.split(\"=\")[0] == \" loss \":\n",
        "            test_loss.append(float(metric.split(\"=\")[1].split(' ')[1]))\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.set_title(hyper_params[\"project_name\"],fontweight=\"bold\", size=20)\n",
        "ax1.plot(test_ndcg, 'b-')\n",
        "ax1.set_xlabel('Epochs', fontsize = 20.0)\n",
        "ax1.set_ylabel('NDCG@100', color='b', fontsize = 20.0)\n",
        "ax1.tick_params('y', colors='b')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(test_loss, 'r--')\n",
        "ax2.set_ylabel('Loss', color='r')\n",
        "ax2.tick_params('y', colors='r')\n",
        "\n",
        "fig.tight_layout()\n",
        "if not os.path.isdir(\"saved_plots/\"): os.mkdir(\"saved_plots/\")\n",
        "fig.savefig(\"saved_plots/learning_curve_\" + hyper_params[\"project_name\"] + \".pdf\")\n",
        "plt.show()\n",
        "\n",
        "# Checking metrics for the test set on best saved model\n",
        "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
        "metrics, len_to_ndcg_at_100_map = evaluate(model, criterion, test_reader, hyper_params, False)\n",
        "\n",
        "# Plot sequence length vs NDCG@100 graph\n",
        "plot_len_vs_ndcg(len_to_ndcg_at_100_map)\n",
        "\n",
        "string = \"\"\n",
        "for m in metrics: string += \" | \" + m + ' = ' + str(metrics[m])\n",
        "\n",
        "ss  = '=' * 89\n",
        "ss += '\\n| End of training'\n",
        "ss += string + \" (TEST)\"\n",
        "ss += '\\n'\n",
        "ss += '=' * 89\n",
        "file_write(hyper_params['log_file'], ss)\n",
        "print(\"average runtime per epoch =\", round(avg_runtime / float(total), 4), \"s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAct2_kz6Ej_",
        "colab_type": "text"
      },
      "source": [
        "# Test Fairness Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pymQT7pgZZWb",
        "colab_type": "text"
      },
      "source": [
        "# backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxGM-8RKeNZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # num_users = 25\n",
        "# num_items = 10\n",
        "# # final_dcg = calcualte_final_dcg(decoder_output_arr2)\n",
        "# topkItens = np.argsort(decoder_output_arr2)[:,-num_items:]\n",
        "# predsn = []\n",
        "# for user in range(topkItens.shape[0]):\n",
        "\n",
        "#    predsn.append(decoder_output_arr2[user,sorted(topkItens[user,:])])\n",
        "# predsn = np.array(predsn)\n",
        "# #print(predsn)\n",
        "\n",
        "# dcg_gt = dcg_k_users(predsn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_eNklLUHBhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGo6YhuhHA3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dcg_k_users(scores):\n",
        "    dcg_round = []\n",
        "    for user in range(scores.shape[0]):\n",
        "        dcg_round.append(dcg_single_ranking(scores[user,:]))\n",
        "    return dcg_round \n",
        "\n",
        "def dcg_single_ranking(scores):\n",
        "    dcg = 0.0\n",
        "    for idx in range(len(scores)):\n",
        "        curr = scores[idx]/np.log2(idx + 2)   \n",
        "        dcg += curr\n",
        "    return dcg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txFTL6Wi2Axv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcualte_final_dcg(decoder_output_arr22):\n",
        "  batch = decoder_output_arr2\n",
        "  round_batch = batch.data[:,0:num_items].cpu().numpy()[0]\n",
        "  print(\"---------------- final_dcg -----------------\")\n",
        "  print(round_batch.size)\n",
        "  print(\"----------------------------------------------\")\n",
        "  final_dcg = dcg_k_users(round_batch)\n",
        "  return final_dcg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXxlxqpfzJ2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Fairness_at_k_rounds(X_pred, dcg_gt):\n",
        "    debug = False\n",
        "    print(\"----------------------- Fairness_at_k_rounds  ->   X_pred -------------------------\")\n",
        "    print(X_pred)\n",
        "    print(\"-----------------------------------------------------------------------------------\")\n",
        "    ufair_all = []\n",
        "    ndcg_all = []\n",
        "    for user in range(X_pred.shape[1]):\n",
        "        ufair, ndcg = 0.0, 0.0\n",
        "        for item in range(X_pred.shape[2]):\n",
        "            sum_a, sum_r = 0,0\n",
        "            item_ufair, item_ndcg = [], []\n",
        "            for iround in range(X_pred.shape[0]):\n",
        "                att, rel = 0,0\n",
        "                if X_pred[iround,user,item] != -np.inf:\n",
        "                    if debug: print(user,item,iround)\n",
        "                    # NORMALIZE SCORES BETWEEN THE MINIMUN AND MAXIMUN VALUES\n",
        "                    #print(min(X_pred[iround,user,:]),max(X_pred[iround,user,:]))\n",
        "                    norm_scores = ((X_pred[iround,user,:] - min(X_pred[iround,user,:]))/(max(X_pred[iround,user,:])- min(X_pred[iround,user,:])))\n",
        "                    #norm_scores = (X_pred[iround,user,:]/max(X_pred[iround,user,:]))\n",
        "                    if debug: print(norm_scores)\n",
        "                    # INDEX OF ITEMS SORTED IN DESCENDING ORDER\n",
        "                    if debug: print(np.argsort(norm_scores)[::-1]) \n",
        "                    # POSITION OF THE CURRENT ITEM\n",
        "                    if debug: print(max(np.argsort(norm_scores)) - np.where(np.argsort(norm_scores) == item)[0][0]) \n",
        "                    att = np.where(np.argsort(norm_scores) == item)[0][0]/max(np.argsort(norm_scores))\n",
        "                    if debug: print('Attention: '+ str(att)) \n",
        "                    sum_a = sum_a + att\n",
        "                    rel = norm_scores[item]\n",
        "                    if debug: print('Relevance: '+str(rel))\n",
        "                    sum_r = sum_r + rel\n",
        "                    if debug: print('Unfairness: '+str(abs(att - rel)))\n",
        "                    if debug: print('DCG: '+str(dcg_single_ranking(X_pred[iround,user,:])))\n",
        "                    if debug: print('NDCG: '+str(dcg_single_ranking(X_pred[iround,user,:])/dcg_gt[user]))\n",
        "                    if debug: input()\n",
        "                    item_ufair.append(abs(att - rel)) \n",
        "                    item_ndcg.append(dcg_single_ranking(X_pred[iround,user,:])/dcg_gt[user])\n",
        "                else: \n",
        "                    print(\"ALERT\")    \n",
        "                    print(user,item,iround) \n",
        "            #plot_curve(item_ufair,item_ndcg)\n",
        "            # SUMS THE UNFAIRNESS OF CURRENT ITEM\n",
        "            ufair = ufair + (abs(sum_a - sum_r)/X_pred.shape[0])\n",
        "            ndcg = ndcg + (np.sum(item_ndcg)/X_pred.shape[0])\n",
        "            if debug: print('Unfairness of Item: '+str((abs(sum_a - sum_r)/X_pred.shape[0])))\n",
        "            if debug: print('Total Current Unfairness: '+str(ufair/(item+1)))\n",
        "            if debug: print('-----------------------------------')\n",
        "        if debug: print('Normalized Total Unfairness: '+str(ufair/X_pred.shape[2]))\n",
        "        if debug: print('Normalized Total NDCG: '+str(ndcg/X_pred.shape[2]))\n",
        "        # normalize by the number of items\n",
        "        ufair_all.append(ufair/X_pred.shape[2])\n",
        "        ndcg_all.append(ndcg/X_pred.shape[2])\n",
        "    return np.array(ufair_all), np.array(ndcg_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13AQBOD-Qu-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_box_color(bp, color):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.setp(bp['boxes'], color=color)\n",
        "    plt.setp(bp['whiskers'], color=color)\n",
        "    plt.setp(bp['caps'], color=color)\n",
        "    plt.setp(bp['medians'], color=color)\n",
        "\n",
        "def plot_comparison(data_a, data_b, ticks, dataset, test_file):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bpl = plt.boxplot(data_a, positions=np.array(range(len(data_a)))*2.0-0.4, sym='', widths=0.6)\n",
        "    bpr = plt.boxplot(data_b, positions=np.array(range(len(data_b)))*2.0+0.4, sym='', widths=0.6)\n",
        "    #bpr = plt.boxplot(data_c, positions=np.array(range(len(data_b)))*2.0+0.4, sym='', widths=0.6)\n",
        "    set_box_color(bpl, '#D7191C') # colors are from http://colorbrewer2.org/\n",
        "    set_box_color(bpr, '#2C7BB6')\n",
        "    #set_box_color(bpr, '#2C7BB6')\n",
        "    \n",
        "    # draw temporary red and blue lines and use them to create a legend\n",
        "    plt.plot([], c='#D7191C', label='Unfairness@100')\n",
        "    plt.plot([], c='#2C7BB6', label='1 - NDCG@100')\n",
        "    #plt.plot([], c='#2C7BB6', label='CNN + STFT')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.xticks(range(0, len(ticks) * 2, 2), ticks)\n",
        "    plt.xlim(-2, len(ticks)*2)\n",
        "    plt.ylim(-0.2,1.00)\n",
        "    #plt.ylim(np.min(np.concatenate((data_a,data_b),axis=1)), np.max(np.concatenate((data_a,data_b),axis=1)))\n",
        "    plt.tight_layout()\n",
        "    from google.colab import files\n",
        "    plt.savefig(\"result.pdf\")\n",
        "    files.download(\"result.pdf\") \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz84XqLi9uy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vqVWtG31no",
        "colab_type": "text"
      },
      "source": [
        "# Fairness Test Eveluation with different noises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqPZ_nvO38ER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1c6b0e7f-4089-4b1e-bb5e-0608f2a1f54d"
      },
      "source": [
        "import numpy\n",
        "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
        "\n",
        "ufairs_arr = []\n",
        "ndcgs_arr = []\n",
        "num_users = 300\n",
        "num_items = 100\n",
        "num_rounds = 10\n",
        "#batch_to_pick = 0 #randint(0, len(decoder_output_arr)-1) #pickes random batch\n",
        "\n",
        "test_reader, total_items = load_data2(hyper_params)\n",
        "final_dcg = None\n",
        "# for s_noise in range(0, 5):\n",
        "hyper_params['total_items'] = total_items\n",
        "hyper_params['testing_batch_limit'] = test_reader.num_b\n",
        "criterion = VAELoss(hyper_params)\n",
        "\n",
        "\n",
        "metrics, len_to_ndcg_at_100_map,decoder_output_arr,decoder_output_arr2 = evaluate1(model, criterion, test_reader, hyper_params, False)\n",
        "#print(decoder_output_arr2.shape)\n",
        "# print('evaluate1') \n",
        "\n",
        "# num_rounds = 10\n",
        "topkItens = np.argsort(decoder_output_arr2)[:,-num_items:]\n",
        "predsn = []\n",
        "for user in range(topkItens.shape[0]):\n",
        "\n",
        "   predsn.append(decoder_output_arr2[user,sorted(topkItens[user,:])])\n",
        "predsn = np.array(predsn)\n",
        "#print(predsn)\n",
        "\n",
        "dcg_gt = dcg_k_users(predsn)\n",
        "\n",
        "print('evaluate2') \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Files loaded!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 571057/571057 [00:01<00:00, 518564.38it/s]\n",
            "100%|██████████| 147726/147726 [00:00<00:00, 861296.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Xtum-n4B1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(hyper_params['model_file_name'], 'rb') as f: model = torch.load(f)\n",
        "\n",
        "ufairs_arr = []\n",
        "ndcgs_arr = []\n",
        "num_users = 300\n",
        "num_items = 100\n",
        "num_rounds = 10\n",
        "\n",
        "test_reader, total_items = load_data2(hyper_params)\n",
        "\n",
        "for s_noise in range(0, 5):\n",
        "\n",
        "  hyper_params['total_items'] = total_items\n",
        "  hyper_params['testing_batch_limit'] = test_reader.num_b\n",
        "  criterion = VAELoss(hyper_params)\n",
        "  metrics, len_to_ndcg_at_100_map,decoder_output_arr,decoder_output_arr2,decoder_output_list  = evaluate2(model, criterion, test_reader, hyper_params, False, num_rounds)\n",
        "  final_pred = decoder_output_list\n",
        "  #print(final_pred.shape)\n",
        "  final_pred = np.array(final_pred)\n",
        "  print(final_pred.shape)\n",
        "  preds_k_rounds_filter = np.zeros((num_rounds,num_users,num_items))\n",
        "  for user in range(num_users):\n",
        "\n",
        "    preds_k_rounds_filter[:,user,:] = final_pred[:,user,sorted(topkItens[user,:])]\n",
        "  preds_k_rounds_filter = np.array(preds_k_rounds_filter)\n",
        "  print(preds_k_rounds_filter.shape) \n",
        "  ufairs, ndcgs = Fairness_at_k_rounds(preds_k_rounds_filter, dcg_gt)\n",
        "  print(\"For Noise = \"+ str(s_noise))\n",
        "  #print(\"Final NDCGs: \",ndcgs)\n",
        "  print(np.mean(ufairs),np.std(ufairs),np.mean(ndcgs),np.std(ndcgs))\n",
        "  #plot_comparison([ufairs],[1-ndcgs],['original'],'dataset','test_file')\n",
        "  ufairs_arr.append(ufairs)\n",
        "  ndcgs_arr.append(ndcgs) \n",
        "\n",
        "# plot_comparison([ufairs_arr[0]],[1-ndcgs_arr[0]],['original'],'dataset','test_file')\n",
        "    \n",
        "\n",
        "\n",
        "# plot_comparison([ufairs_arr[1]],[1-ndcgs_arr[1]],['N(std=0.5)'],'dataset','test_file')\n",
        "\n",
        "\n",
        "\n",
        "# plot_comparison([ufairs_arr[2]],[1-ndcgs_arr[2]],['N(std=1)'],'dataset','test_file')\n",
        "\n",
        "\n",
        "\n",
        "# plot_comparison([ufairs_arr[3]],[1-ndcgs_arr[3]],['N(std=2)'],'dataset','test_file')\n",
        "\n",
        "\n",
        "\n",
        "# plot_comparison([ufairs_arr[4]],[1-ndcgs_arr[4]],['uniform'],'dataset','test_file')  \n",
        "\n",
        "#plot_comparison([ufairs_arr[0],ufairs_arr[1], ufairs_arr[2], ufairs_arr[3], ufairs_arr[4]],[1-ndcgs_arr[0],1-ndcgs_arr[1], 1-ndcgs_arr[2],1-ndcgs_arr[3],1-ndcgs_arr[4]],['original','N(std=0.5)','N(std=1.0)','N(std=2.0)','uniform'], 'dataset', 'test_file')\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4WXkCBo57jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "c022bbc5-611d-4ad9-c3a3-39a2ff9ad0e8"
      },
      "source": [
        "plot_comparison([ufairs_arr[0],ufairs_arr[1], ufairs_arr[2], ufairs_arr[3], ufairs_arr[4]],[1-ndcgs_arr[0],1-ndcgs_arr[1], 1-ndcgs_arr[2],1-ndcgs_arr[3],1-ndcgs_arr[4]],['original','N(std=0.5)','N(std=1.0)','N(std=2.0)','uniform'], 'dataset', 'test_file')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7SEpsaU9D5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}